{
  "model": "Qwen__Qwen3-Omni-30B-A3B-Instruct",
  "benchmark_id": "mmlu_redux_machine_learning",
  "created_at": "2025-10-31T01:00:13.111Z",
  "metrics": {
    "results": {
      "mmlu_redux_all": {
        "acc,none": 0.7590000000000001,
        "acc_stderr,none": 0.00754641455361679,
        "alias": "mmlu_redux_all"
      },
      "mmlu_redux_anatomy": {
        "alias": " - mmlu_redux_anatomy",
        "acc,none": 0.8,
        "acc_stderr,none": 0.04020151261036844
      },
      "mmlu_redux_astronomy": {
        "alias": " - mmlu_redux_astronomy",
        "acc,none": 0.93,
        "acc_stderr,none": 0.02564323999762429
      },
      "mmlu_redux_business_ethics": {
        "alias": " - mmlu_redux_business_ethics",
        "acc,none": 0.83,
        "acc_stderr,none": 0.0377525168068637
      },
      "mmlu_redux_clinical_knowledge": {
        "alias": " - mmlu_redux_clinical_knowledge",
        "acc,none": 0.85,
        "acc_stderr,none": 0.03588702812826371
      },
      "mmlu_redux_college_chemistry": {
        "alias": " - mmlu_redux_college_chemistry",
        "acc,none": 0.62,
        "acc_stderr,none": 0.04878317312145629
      },
      "mmlu_redux_college_computer_science": {
        "alias": " - mmlu_redux_college_computer_science",
        "acc,none": 0.79,
        "acc_stderr,none": 0.040936018074033256
      },
      "mmlu_redux_college_mathematics": {
        "alias": " - mmlu_redux_college_mathematics",
        "acc,none": 0.66,
        "acc_stderr,none": 0.04760952285695237
      },
      "mmlu_redux_college_medicine": {
        "alias": " - mmlu_redux_college_medicine",
        "acc,none": 0.82,
        "acc_stderr,none": 0.03861229196653694
      },
      "mmlu_redux_college_physics": {
        "alias": " - mmlu_redux_college_physics",
        "acc,none": 0.65,
        "acc_stderr,none": 0.04793724854411021
      },
      "mmlu_redux_conceptual_physics": {
        "alias": " - mmlu_redux_conceptual_physics",
        "acc,none": 0.88,
        "acc_stderr,none": 0.03265986323710905
      },
      "mmlu_redux_econometrics": {
        "alias": " - mmlu_redux_econometrics",
        "acc,none": 0.68,
        "acc_stderr,none": 0.046882617226215034
      },
      "mmlu_redux_electrical_engineering": {
        "alias": " - mmlu_redux_electrical_engineering",
        "acc,none": 0.8,
        "acc_stderr,none": 0.04020151261036843
      },
      "mmlu_redux_formal_logic": {
        "alias": " - mmlu_redux_formal_logic",
        "acc,none": 0.73,
        "acc_stderr,none": 0.044619604333847394
      },
      "mmlu_redux_global_facts": {
        "alias": " - mmlu_redux_global_facts",
        "acc,none": 0.49,
        "acc_stderr,none": 0.05024183937956912
      },
      "mmlu_redux_high_school_chemistry": {
        "alias": " - mmlu_redux_high_school_chemistry",
        "acc,none": 0.79,
        "acc_stderr,none": 0.040936018074033256
      },
      "mmlu_redux_high_school_geography": {
        "alias": " - mmlu_redux_high_school_geography",
        "acc,none": 0.88,
        "acc_stderr,none": 0.03265986323710906
      },
      "mmlu_redux_high_school_macroeconomics": {
        "alias": " - mmlu_redux_high_school_macroeconomics",
        "acc,none": 0.87,
        "acc_stderr,none": 0.03379976689896309
      },
      "mmlu_redux_high_school_mathematics": {
        "alias": " - mmlu_redux_high_school_mathematics",
        "acc,none": 0.62,
        "acc_stderr,none": 0.04878317312145632
      },
      "mmlu_redux_high_school_physics": {
        "alias": " - mmlu_redux_high_school_physics",
        "acc,none": 0.74,
        "acc_stderr,none": 0.04408440022768079
      },
      "mmlu_redux_high_school_statistics": {
        "alias": " - mmlu_redux_high_school_statistics",
        "acc,none": 0.82,
        "acc_stderr,none": 0.03861229196653695
      },
      "mmlu_redux_high_school_us_history": {
        "alias": " - mmlu_redux_high_school_us_history",
        "acc,none": 0.94,
        "acc_stderr,none": 0.023868325657594183
      },
      "mmlu_redux_human_aging": {
        "alias": " - mmlu_redux_human_aging",
        "acc,none": 0.77,
        "acc_stderr,none": 0.04229525846816506
      },
      "mmlu_redux_logical_fallacies": {
        "alias": " - mmlu_redux_logical_fallacies",
        "acc,none": 0.88,
        "acc_stderr,none": 0.032659863237109066
      },
      "mmlu_redux_machine_learning": {
        "alias": " - mmlu_redux_machine_learning",
        "acc,none": 0.69,
        "acc_stderr,none": 0.04648231987117316
      },
      "mmlu_redux_miscellaneous": {
        "alias": " - mmlu_redux_miscellaneous",
        "acc,none": 0.9,
        "acc_stderr,none": 0.030151134457776348
      },
      "mmlu_redux_philosophy": {
        "alias": " - mmlu_redux_philosophy",
        "acc,none": 0.79,
        "acc_stderr,none": 0.040936018074033256
      },
      "mmlu_redux_professional_accounting": {
        "alias": " - mmlu_redux_professional_accounting",
        "acc,none": 0.67,
        "acc_stderr,none": 0.047258156262526094
      },
      "mmlu_redux_professional_law": {
        "alias": " - mmlu_redux_professional_law",
        "acc,none": 0.65,
        "acc_stderr,none": 0.04793724854411022
      },
      "mmlu_redux_public_relations": {
        "alias": " - mmlu_redux_public_relations",
        "acc,none": 0.76,
        "acc_stderr,none": 0.04292346959909284
      },
      "mmlu_redux_virology": {
        "alias": " - mmlu_redux_virology",
        "acc,none": 0.47,
        "acc_stderr,none": 0.05016135580465921
      }
    },
    "groups": {
      "mmlu_redux_all": {
        "acc,none": 0.7590000000000001,
        "acc_stderr,none": 0.00754641455361679,
        "alias": "mmlu_redux_all"
      }
    },
    "group_subtasks": {
      "mmlu_redux_all": [
        "mmlu_redux_anatomy",
        "mmlu_redux_astronomy",
        "mmlu_redux_business_ethics",
        "mmlu_redux_clinical_knowledge",
        "mmlu_redux_college_chemistry",
        "mmlu_redux_college_computer_science",
        "mmlu_redux_college_mathematics",
        "mmlu_redux_college_medicine",
        "mmlu_redux_college_physics",
        "mmlu_redux_econometrics",
        "mmlu_redux_electrical_engineering",
        "mmlu_redux_formal_logic",
        "mmlu_redux_global_facts",
        "mmlu_redux_high_school_chemistry",
        "mmlu_redux_high_school_mathematics",
        "mmlu_redux_high_school_physics",
        "mmlu_redux_high_school_statistics",
        "mmlu_redux_human_aging",
        "mmlu_redux_logical_fallacies",
        "mmlu_redux_machine_learning",
        "mmlu_redux_miscellaneous",
        "mmlu_redux_philosophy",
        "mmlu_redux_professional_accounting",
        "mmlu_redux_public_relations",
        "mmlu_redux_virology",
        "mmlu_redux_conceptual_physics",
        "mmlu_redux_high_school_us_history",
        "mmlu_redux_high_school_geography",
        "mmlu_redux_high_school_macroeconomics",
        "mmlu_redux_professional_law"
      ]
    },
    "configs": {
      "mmlu_redux_anatomy": {
        "task": "mmlu_redux_anatomy",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "anatomy",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bfbac0>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb6710>",
        "description": "MMLU Redux 2.0 (Anatomy) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_astronomy": {
        "task": "mmlu_redux_astronomy",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "astronomy",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bfb880>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bfbc70>",
        "description": "MMLU Redux 2.0 (mmlu_redux_astronomy) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_business_ethics": {
        "task": "mmlu_redux_business_ethics",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "business_ethics",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bfa830>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bf8d30>",
        "description": "MMLU Redux 2.0 (mmlu_redux_business_ethics) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_clinical_knowledge": {
        "task": "mmlu_redux_clinical_knowledge",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "clinical_knowledge",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bfa3b0>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bf88b0>",
        "description": "MMLU Redux 2.0 (mmlu_rmmlu_redux_clinical_knowledgeedux_astronomy) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_college_chemistry": {
        "task": "mmlu_redux_college_chemistry",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "college_chemistry",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bfaa70>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bfa7a0>",
        "description": "MMLU Redux 2.0 (mmlu_redux_college_chemistry) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_college_computer_science": {
        "task": "mmlu_redux_college_computer_science",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "college_computer_science",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bfac20>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bfb0a0>",
        "description": "MMLU Redux 2.0 (mmlu_redux_college_computer_science) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_college_mathematics": {
        "task": "mmlu_redux_college_mathematics",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "college_mathematics",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bfa680>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bf96c0>",
        "description": "MMLU Redux 2.0 (mmlu_redux_college_mathematics) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_college_medicine": {
        "task": "mmlu_redux_college_medicine",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "college_medicine",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bfa0e0>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bf9510>",
        "description": "MMLU Redux 2.0 (mmlu_redux_college_medicine) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_college_physics": {
        "task": "mmlu_redux_college_physics",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "college_physics",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bf9d80>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bf8280>",
        "description": "MMLU Redux 2.0 (mmlu_redux_college_physics) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_conceptual_physics": {
        "task": "mmlu_redux_conceptual_physics",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "conceptual_physics",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155144d33010>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155144d31360>",
        "description": "MMLU Redux 2.0 (mmlu_redux_conceptual_physics) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_econometrics": {
        "task": "mmlu_redux_econometrics",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "econometrics",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bf80d0>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bf97e0>",
        "description": "MMLU Redux 2.0 (mmlu_reduxmmlu_redux_econometrics_conceptual_physics) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_electrical_engineering": {
        "task": "mmlu_redux_electrical_engineering",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "electrical_engineering",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bf9360>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bf8ee0>",
        "description": "MMLU Redux 2.0 (mmlu_redux_electrical_engineering) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_formal_logic": {
        "task": "mmlu_redux_formal_logic",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "formal_logic",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x1551575c6c20>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bf8e50>",
        "description": "MMLU Redux 2.0 (mmlu_redux_formal_logic) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_global_facts": {
        "task": "mmlu_redux_global_facts",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "global_facts",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb6830>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb6e60>",
        "description": "MMLU Redux 2.0 (mmlu_redux_global_facts) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_high_school_chemistry": {
        "task": "mmlu_redux_high_school_chemistry",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "high_school_chemistry",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb5870>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb7ac0>",
        "description": "MMLU Redux 2.0 (mmlu_redux_high_school_chemistry) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_high_school_geography": {
        "task": "mmlu_redux_high_school_geography",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "high_school_geography",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155144d33250>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155144d32950>",
        "description": "MMLU Redux 2.0 (mmlu_redux_high_school_geography) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_high_school_macroeconomics": {
        "task": "mmlu_redux_high_school_macroeconomics",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "high_school_macroeconomics",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155144d31c60>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155144d32440>",
        "description": "MMLU Redux 2.0 (mmlu_redux_high_school_macroeconomics) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_high_school_mathematics": {
        "task": "mmlu_redux_high_school_mathematics",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "high_school_mathematics",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb77f0>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb7640>",
        "description": "MMLU Redux 2.0 (mmlu_redux_high_school_mathematics) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_high_school_physics": {
        "task": "mmlu_redux_high_school_physics",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "high_school_physics",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb6f80>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb6b00>",
        "description": "MMLU Redux 2.0 (mmlu_redux_high_school_physics) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_high_school_statistics": {
        "task": "mmlu_redux_high_school_statistics",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "high_school_statistics",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb5750>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb4820>",
        "description": "MMLU Redux 2.0 (mmlu_redux_high_school_statistics) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_high_school_us_history": {
        "task": "mmlu_redux_high_school_us_history",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "high_school_us_history",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155144d33be0>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155144d32ef0>",
        "description": "MMLU Redux 2.0 (mmlu_redux_high_school_us_history) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_human_aging": {
        "task": "mmlu_redux_human_aging",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "human_aging",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb5000>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb4430>",
        "description": "MMLU Redux 2.0 (mmlu_redux_human_aging) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_logical_fallacies": {
        "task": "mmlu_redux_logical_fallacies",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "logical_fallacies",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x1551450ddc60>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb43a0>",
        "description": "MMLU Redux 2.0 (mmlu_redux_logical_fallacies) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_machine_learning": {
        "task": "mmlu_redux_machine_learning",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "machine_learning",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb6950>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb5e10>",
        "description": "MMLU Redux 2.0 (mmlu_redux_machine_learning) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_miscellaneous": {
        "task": "mmlu_redux_miscellaneous",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "miscellaneous",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb5900>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb4670>",
        "description": "MMLU Redux 2.0 (mmlu_redux_miscellaneous) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_philosophy": {
        "task": "mmlu_redux_philosophy",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "philosophy",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb5d80>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb5a20>",
        "description": "MMLU Redux 2.0 (mmlu_redux_philosophy) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_professional_accounting": {
        "task": "mmlu_redux_professional_accounting",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "professional_accounting",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb5510>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb4280>",
        "description": "MMLU Redux 2.0 (mmlu_redux_professional_accounting) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_professional_law": {
        "task": "mmlu_redux_professional_law",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "professional_law",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155145a8dab0>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155145a8ec20>",
        "description": "MMLU Redux 2.0 (mmlu_redux_professional_law) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_public_relations": {
        "task": "mmlu_redux_public_relations",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "public_relations",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155143bb4ee0>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155143bb4b80>",
        "description": "MMLU Redux 2.0 (mmlu_redux_public_relations) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      },
      "mmlu_redux_virology": {
        "task": "mmlu_redux_virology",
        "dataset_path": "edinburgh-dawg/mmlu-redux",
        "dataset_name": "virology",
        "test_split": "test",
        "full_docs": false,
        "process_results_use_image": false,
        "doc_to_visual": "<function no_visuals at 0x155144d33c70>",
        "doc_to_text": "Answer the following multiple-choice question by providing only the letter (A, B, C, or D) corresponding to the correct option.\n\nQuestion: {{question}}\n\nChoices:\nA) {{choices[0]}}\nB) {{choices[1]}}\nC) {{choices[2]}}\nD) {{choices[3]}}\n\nAnswer:",
        "doc_to_target": "{{answer}}",
        "process_results": "<function process_mmlu_choice at 0x155144d33130>",
        "description": "MMLU Redux 2.0 (mmlu_redux_virology) - Generative Multiple Choice.",
        "target_delimiter": " ",
        "fewshot_delimiter": "\n\n",
        "num_fewshot": 0,
        "metric_list": [
          {
            "metric": "acc",
            "aggregation": "mean",
            "higher_is_better": true
          }
        ],
        "output_type": "generate_until",
        "generation_kwargs": {
          "max_new_tokens": 3,
          "temperature": 0,
          "until": [
            "\n",
            ".",
            ","
          ]
        },
        "repeats": 1,
        "should_decontaminate": false
      }
    },
    "versions": {
      "mmlu_redux_all": 1,
      "mmlu_redux_anatomy": "Yaml",
      "mmlu_redux_astronomy": "Yaml",
      "mmlu_redux_business_ethics": "Yaml",
      "mmlu_redux_clinical_knowledge": "Yaml",
      "mmlu_redux_college_chemistry": "Yaml",
      "mmlu_redux_college_computer_science": "Yaml",
      "mmlu_redux_college_mathematics": "Yaml",
      "mmlu_redux_college_medicine": "Yaml",
      "mmlu_redux_college_physics": "Yaml",
      "mmlu_redux_conceptual_physics": "Yaml",
      "mmlu_redux_econometrics": "Yaml",
      "mmlu_redux_electrical_engineering": "Yaml",
      "mmlu_redux_formal_logic": "Yaml",
      "mmlu_redux_global_facts": "Yaml",
      "mmlu_redux_high_school_chemistry": "Yaml",
      "mmlu_redux_high_school_geography": "Yaml",
      "mmlu_redux_high_school_macroeconomics": "Yaml",
      "mmlu_redux_high_school_mathematics": "Yaml",
      "mmlu_redux_high_school_physics": "Yaml",
      "mmlu_redux_high_school_statistics": "Yaml",
      "mmlu_redux_high_school_us_history": "Yaml",
      "mmlu_redux_human_aging": "Yaml",
      "mmlu_redux_logical_fallacies": "Yaml",
      "mmlu_redux_machine_learning": "Yaml",
      "mmlu_redux_miscellaneous": "Yaml",
      "mmlu_redux_philosophy": "Yaml",
      "mmlu_redux_professional_accounting": "Yaml",
      "mmlu_redux_professional_law": "Yaml",
      "mmlu_redux_public_relations": "Yaml",
      "mmlu_redux_virology": "Yaml"
    },
    "n-shot": {
      "mmlu_redux_anatomy": 0,
      "mmlu_redux_astronomy": 0,
      "mmlu_redux_business_ethics": 0,
      "mmlu_redux_clinical_knowledge": 0,
      "mmlu_redux_college_chemistry": 0,
      "mmlu_redux_college_computer_science": 0,
      "mmlu_redux_college_mathematics": 0,
      "mmlu_redux_college_medicine": 0,
      "mmlu_redux_college_physics": 0,
      "mmlu_redux_conceptual_physics": 0,
      "mmlu_redux_econometrics": 0,
      "mmlu_redux_electrical_engineering": 0,
      "mmlu_redux_formal_logic": 0,
      "mmlu_redux_global_facts": 0,
      "mmlu_redux_high_school_chemistry": 0,
      "mmlu_redux_high_school_geography": 0,
      "mmlu_redux_high_school_macroeconomics": 0,
      "mmlu_redux_high_school_mathematics": 0,
      "mmlu_redux_high_school_physics": 0,
      "mmlu_redux_high_school_statistics": 0,
      "mmlu_redux_high_school_us_history": 0,
      "mmlu_redux_human_aging": 0,
      "mmlu_redux_logical_fallacies": 0,
      "mmlu_redux_machine_learning": 0,
      "mmlu_redux_miscellaneous": 0,
      "mmlu_redux_philosophy": 0,
      "mmlu_redux_professional_accounting": 0,
      "mmlu_redux_professional_law": 0,
      "mmlu_redux_public_relations": 0,
      "mmlu_redux_virology": 0
    },
    "higher_is_better": {
      "mmlu_redux_all": {
        "acc": true
      },
      "mmlu_redux_anatomy": {
        "acc": true
      },
      "mmlu_redux_astronomy": {
        "acc": true
      },
      "mmlu_redux_business_ethics": {
        "acc": true
      },
      "mmlu_redux_clinical_knowledge": {
        "acc": true
      },
      "mmlu_redux_college_chemistry": {
        "acc": true
      },
      "mmlu_redux_college_computer_science": {
        "acc": true
      },
      "mmlu_redux_college_mathematics": {
        "acc": true
      },
      "mmlu_redux_college_medicine": {
        "acc": true
      },
      "mmlu_redux_college_physics": {
        "acc": true
      },
      "mmlu_redux_conceptual_physics": {
        "acc": true
      },
      "mmlu_redux_econometrics": {
        "acc": true
      },
      "mmlu_redux_electrical_engineering": {
        "acc": true
      },
      "mmlu_redux_formal_logic": {
        "acc": true
      },
      "mmlu_redux_global_facts": {
        "acc": true
      },
      "mmlu_redux_high_school_chemistry": {
        "acc": true
      },
      "mmlu_redux_high_school_geography": {
        "acc": true
      },
      "mmlu_redux_high_school_macroeconomics": {
        "acc": true
      },
      "mmlu_redux_high_school_mathematics": {
        "acc": true
      },
      "mmlu_redux_high_school_physics": {
        "acc": true
      },
      "mmlu_redux_high_school_statistics": {
        "acc": true
      },
      "mmlu_redux_high_school_us_history": {
        "acc": true
      },
      "mmlu_redux_human_aging": {
        "acc": true
      },
      "mmlu_redux_logical_fallacies": {
        "acc": true
      },
      "mmlu_redux_machine_learning": {
        "acc": true
      },
      "mmlu_redux_miscellaneous": {
        "acc": true
      },
      "mmlu_redux_philosophy": {
        "acc": true
      },
      "mmlu_redux_professional_accounting": {
        "acc": true
      },
      "mmlu_redux_professional_law": {
        "acc": true
      },
      "mmlu_redux_public_relations": {
        "acc": true
      },
      "mmlu_redux_virology": {
        "acc": true
      }
    },
    "n-samples": {
      "mmlu_redux_anatomy": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_astronomy": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_business_ethics": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_clinical_knowledge": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_college_chemistry": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_college_computer_science": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_college_mathematics": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_college_medicine": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_college_physics": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_econometrics": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_electrical_engineering": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_formal_logic": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_global_facts": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_high_school_chemistry": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_high_school_mathematics": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_high_school_physics": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_high_school_statistics": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_human_aging": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_logical_fallacies": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_machine_learning": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_miscellaneous": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_philosophy": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_professional_accounting": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_public_relations": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_virology": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_conceptual_physics": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_high_school_us_history": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_high_school_geography": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_high_school_macroeconomics": {
        "original": 100,
        "effective": 100
      },
      "mmlu_redux_professional_law": {
        "original": 100,
        "effective": 100
      }
    },
    "config": {
      "model": "vllm",
      "model_args": "model=Qwen/Qwen3-Omni-30B-A3B-Instruct,tensor_parallel_size=4,gpu_memory_utilization=0.9,allowed_local_media_path=/",
      "batch_size": "64",
      "batch_sizes": [],
      "device": null,
      "use_cache": null,
      "limit": null,
      "bootstrap_iters": 100000,
      "gen_kwargs": "",
      "random_seed": 0,
      "numpy_seed": 1234,
      "torch_seed": 1234,
      "fewshot_seed": 1234
    },
    "git_hash": "bbd344e",
    "date": "20251030_142921",
    "task_hashes": {
      "mmlu_redux_anatomy": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_astronomy": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_business_ethics": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_clinical_knowledge": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_college_chemistry": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_college_computer_science": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_college_mathematics": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_college_medicine": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_college_physics": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_econometrics": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_electrical_engineering": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_formal_logic": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_global_facts": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_high_school_chemistry": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_high_school_mathematics": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_high_school_physics": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_high_school_statistics": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_human_aging": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_logical_fallacies": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_machine_learning": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_miscellaneous": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_philosophy": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_professional_accounting": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_public_relations": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_virology": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_conceptual_physics": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_high_school_us_history": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_high_school_geography": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_high_school_macroeconomics": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3",
      "mmlu_redux_professional_law": "be2f5d66ce49b6fe06c8bcda93d223c64132f082403f3a29556444c126126df3"
    },
    "model_source": "vllm",
    "model_name": "Qwen/Qwen3-Omni-30B-A3B-Instruct",
    "model_name_sanitized": "Qwen__Qwen3-Omni-30B-A3B-Instruct",
    "system_instruction": null,
    "system_instruction_sha": null,
    "fewshot_as_multiturn": false,
    "chat_template": null,
    "chat_template_sha": null,
    "start_time": 4906555.853261716,
    "end_time": 4906807.368988938,
    "total_evaluation_time_seconds": "251.5157272219658"
  },
  "raw_files": [
    {
      "filename": "20251030_142921_results.json",
      "absolute_path": "C:\\Users\\Kata\\Desktop\\intern\\gui-test-suite\\results\\Qwen__Qwen3-Omni-30B-A3B-Instruct\\Qwen__Qwen3-Omni-30B-A3B-Instruct\\20251030_142921_results.json"
    },
    {
      "filename": "20251030_142921_samples_mmlu_redux_machine_learning.jsonl",
      "absolute_path": "C:\\Users\\Kata\\Desktop\\intern\\gui-test-suite\\results\\Qwen__Qwen3-Omni-30B-A3B-Instruct\\Qwen__Qwen3-Omni-30B-A3B-Instruct\\20251030_142921_samples_mmlu_redux_machine_learning.jsonl"
    }
  ]
}